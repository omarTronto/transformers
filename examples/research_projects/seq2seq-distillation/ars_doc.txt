usage: finetune.py [-h] [--logger [LOGGER]] [--checkpoint_callback [CHECKPOINT_CALLBACK]] [--default_root_dir DEFAULT_ROOT_DIR]
                   [--gradient_clip_val GRADIENT_CLIP_VAL] [--process_position PROCESS_POSITION] [--num_nodes NUM_NODES]
                   [--num_processes NUM_PROCESSES] [--gpus GPUS] [--auto_select_gpus [AUTO_SELECT_GPUS]] [--tpu_cores TPU_CORES]
                   [--log_gpu_memory LOG_GPU_MEMORY] [--progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE]
                   [--overfit_batches OVERFIT_BATCHES] [--track_grad_norm TRACK_GRAD_NORM]
                   [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH] [--fast_dev_run [FAST_DEV_RUN]]
                   [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES] [--max_epochs MAX_EPOCHS] [--min_epochs MIN_EPOCHS]
                   [--max_steps MAX_STEPS] [--min_steps MIN_STEPS] [--limit_train_batches LIMIT_TRAIN_BATCHES]
                   [--limit_val_batches LIMIT_VAL_BATCHES] [--limit_test_batches LIMIT_TEST_BATCHES]
                   [--val_check_interval VAL_CHECK_INTERVAL] [--flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS]
                   [--log_every_n_steps LOG_EVERY_N_STEPS] [--accelerator ACCELERATOR] [--sync_batchnorm [SYNC_BATCHNORM]]
                   [--precision PRECISION] [--weights_summary WEIGHTS_SUMMARY] [--weights_save_path WEIGHTS_SAVE_PATH]
                   [--num_sanity_val_steps NUM_SANITY_VAL_STEPS] [--truncated_bptt_steps TRUNCATED_BPTT_STEPS]
                   [--resume_from_checkpoint RESUME_FROM_CHECKPOINT] [--profiler [PROFILER]] [--benchmark [BENCHMARK]]
                   [--deterministic [DETERMINISTIC]] [--reload_dataloaders_every_epoch [RELOAD_DATALOADERS_EVERY_EPOCH]]
                   [--auto_lr_find [AUTO_LR_FIND]] [--replace_sampler_ddp [REPLACE_SAMPLER_DDP]] [--terminate_on_nan [TERMINATE_ON_NAN]]
                   [--auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]] [--prepare_data_per_node [PREPARE_DATA_PER_NODE]]
                   [--amp_backend AMP_BACKEND] [--amp_level AMP_LEVEL] [--distributed_backend DISTRIBUTED_BACKEND]
                   [--automatic_optimization [AUTOMATIC_OPTIMIZATION]] --model_name_or_path MODEL_NAME_OR_PATH [--config_name CONFIG_NAME]
                   [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR] [--encoder_layerdrop ENCODER_LAYERDROP]
                   [--decoder_layerdrop DECODER_LAYERDROP] [--dropout DROPOUT] [--attention_dropout ATTENTION_DROPOUT]
                   [--learning_rate LEARNING_RATE] [--lr_scheduler {cosine, cosine_w_restarts, linear, polynomial}]
                   [--weight_decay WEIGHT_DECAY] [--adam_epsilon ADAM_EPSILON] [--warmup_steps WARMUP_STEPS] [--num_workers NUM_WORKERS]
                   [--num_train_epochs MAX_EPOCHS] [--train_batch_size TRAIN_BATCH_SIZE] [--eval_batch_size EVAL_BATCH_SIZE] [--adafactor]
                   --output_dir OUTPUT_DIR [--fp16] [--fp16_opt_level FP16_OPT_LEVEL] [--n_tpu_cores TPU_CORES]
                   [--max_grad_norm GRADIENT_CLIP_VAL] [--do_train] [--do_predict] [--gradient_accumulation_steps ACCUMULATE_GRAD_BATCHES]
                   [--seed SEED] --data_dir DATA_DIR [--max_source_length MAX_SOURCE_LENGTH] [--max_target_length MAX_TARGET_LENGTH]
                   [--val_max_target_length VAL_MAX_TARGET_LENGTH] [--test_max_target_length TEST_MAX_TARGET_LENGTH] [--freeze_encoder]
                   [--freeze_embeds] [--sortish_sampler] [--overwrite_output_dir] [--max_tokens_per_batch MAX_TOKENS_PER_BATCH]
                   [--logger_name {default,wandb,wandb_shared}] [--n_train N_TRAIN] [--n_val N_VAL] [--n_test N_TEST] [--task TASK]
                   [--label_smoothing LABEL_SMOOTHING] [--src_lang SRC_LANG] [--tgt_lang TGT_LANG] [--eval_beams EVAL_BEAMS]
                   [--val_metric {bleu,rouge2,loss,None}] [--eval_max_gen_length EVAL_MAX_GEN_LENGTH] [--save_top_k SAVE_TOP_K]
                   [--early_stopping_patience EARLY_STOPPING_PATIENCE]

optional arguments:
  -h, --help            show this help message and exit
  --logger [LOGGER]     Logger (or iterable collection of loggers) for experiment tracking.
  --checkpoint_callback [CHECKPOINT_CALLBACK]
                        Callback for checkpointing.
  --default_root_dir DEFAULT_ROOT_DIR
                        Default path for logs and weights when no logger/ckpt_callback passed. Default: ``os.getcwd()``. Can be remote
                        file paths such as `s3://mybucket/path` or 'hdfs://path/'
  --gradient_clip_val GRADIENT_CLIP_VAL
                        0 means don't clip.
  --process_position PROCESS_POSITION
                        orders the progress bar when running multiple models on same machine.
  --num_nodes NUM_NODES
                        number of GPU nodes for distributed training.
  --num_processes NUM_PROCESSES
  --gpus GPUS           number of gpus to train on (int) or which GPUs to train on (list or str) applied per node
  --auto_select_gpus [AUTO_SELECT_GPUS]
                        If enabled and `gpus` is an integer, pick available gpus automatically. This is especially useful when GPUs are
                        configured to be in "exclusive mode", such that only one process at a time can access them.
  --tpu_cores TPU_CORES
                        How many TPU cores to train on (1 or 8) / Single TPU to train on [1]
  --log_gpu_memory LOG_GPU_MEMORY
                        None, 'min_max', 'all'. Might slow performance
  --progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE
                        How often to refresh progress bar (in steps). Value ``0`` disables progress bar. Ignored when a custom callback is
                        passed to :paramref:`~Trainer.callbacks`.
  --overfit_batches OVERFIT_BATCHES
                        Overfit a percent of training data (float) or a set number of batches (int). Default: 0.0
  --track_grad_norm TRACK_GRAD_NORM
                        -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm.
  --check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH
                        Check val every n train epochs.
  --fast_dev_run [FAST_DEV_RUN]
                        runs 1 batch of train, test and val to find any bugs (ie: a sort of unit test).
  --accumulate_grad_batches ACCUMULATE_GRAD_BATCHES
                        Accumulates grads every k batches or as set up in the dict.
  --max_epochs MAX_EPOCHS
                        Stop training once this number of epochs is reached.
  --min_epochs MIN_EPOCHS
                        Force training for at least these many epochs
  --max_steps MAX_STEPS
                        Stop training after this number of steps. Disabled by default (None).
  --min_steps MIN_STEPS
                        Force training for at least these number of steps. Disabled by default (None).
  --limit_train_batches LIMIT_TRAIN_BATCHES
                        How much of training dataset to check (floats = percent, int = num_batches)
  --limit_val_batches LIMIT_VAL_BATCHES
                        How much of validation dataset to check (floats = percent, int = num_batches)
  --limit_test_batches LIMIT_TEST_BATCHES
                        How much of test dataset to check (floats = percent, int = num_batches)
  --val_check_interval VAL_CHECK_INTERVAL
                        How often to check the validation set. Use float to check within a training epoch, use int to check every n steps
                        (batches).
  --flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS
                        How often to flush logs to disk (defaults to every 100 steps).
  --log_every_n_steps LOG_EVERY_N_STEPS
                        How often to log within steps (defaults to every 50 steps).
  --accelerator ACCELERATOR
                        Previously known as distributed_backend (dp, ddp, ddp2, etc...). Can also take in an accelerator object for custom
                        hardware.
  --sync_batchnorm [SYNC_BATCHNORM]
                        Synchronize batch norm layers between process groups/whole world.
  --precision PRECISION
                        Full precision (32), half precision (16). Can be used on CPU, GPU or TPUs.
  --weights_summary WEIGHTS_SUMMARY
                        Prints a summary of the weights when training begins.
  --weights_save_path WEIGHTS_SAVE_PATH
                        Where to save weights if specified. Will override default_root_dir for checkpoints only. Use this if for whatever
                        reason you need the checkpoints stored in a different place than the logs written in `default_root_dir`. Can be
                        remote file paths such as `s3://mybucket/path` or 'hdfs://path/' Defaults to `default_root_dir`.
  --num_sanity_val_steps NUM_SANITY_VAL_STEPS
                        Sanity check runs n validation batches before starting the training routine. Set it to `-1` to run all batches in
                        all validation dataloaders. Default: 2
  --truncated_bptt_steps TRUNCATED_BPTT_STEPS
                        Truncated back prop breaks performs backprop every k steps of much longer sequence.
  --resume_from_checkpoint RESUME_FROM_CHECKPOINT
                        To resume training from a specific checkpoint pass in the path here. This can be a URL.
  --profiler [PROFILER]
                        To profile individual steps during training and assist in identifying bottlenecks. Passing bool value is
                        deprecated in v1.1 and will be removed in v1.3.
  --benchmark [BENCHMARK]
                        If true enables cudnn.benchmark.
  --deterministic [DETERMINISTIC]
                        If true enables cudnn.deterministic.
  --reload_dataloaders_every_epoch [RELOAD_DATALOADERS_EVERY_EPOCH]
                        Set to True to reload dataloaders every epoch.
  --auto_lr_find [AUTO_LR_FIND]
                        If set to True, will make trainer.tune() run a learning rate finder, trying to optimize initial learning for
                        faster convergence. trainer.tune() method will set the suggested learning rate in self.lr or self.learning_rate in
                        the LightningModule. To use a different key set a string instead of True with the key name.
  --replace_sampler_ddp [REPLACE_SAMPLER_DDP]
                        Explicitly enables or disables sampler replacement. If not specified this will toggled automatically when DDP is
                        used. By default it will add ``shuffle=True`` for train sampler and ``shuffle=False`` for val/test sampler. If you
                        want to customize it, you can set ``replace_sampler_ddp=False`` and add your own distributed sampler.
  --terminate_on_nan [TERMINATE_ON_NAN]
                        If set to True, will terminate training (by raising a `ValueError`) at the end of each training batch, if any of
                        the parameters or the loss are NaN or +/-inf.
  --auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]
                        If set to True, will `initially` run a batch size finder trying to find the largest batch size that fits into
                        memory. The result will be stored in self.batch_size in the LightningModule. Additionally, can be set to either
                        `power` that estimates the batch size through a power search or `binsearch` that estimates the batch size through
                        a binary search.
  --prepare_data_per_node [PREPARE_DATA_PER_NODE]
                        If True, each LOCAL_RANK=0 will call prepare data. Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data
  --amp_backend AMP_BACKEND
                        The mixed precision backend to use ("native" or "apex")
  --amp_level AMP_LEVEL
                        The optimization level to use (O1, O2, etc...).
  --distributed_backend DISTRIBUTED_BACKEND
                        deprecated. Please use 'accelerator'
  --automatic_optimization [AUTOMATIC_OPTIMIZATION]
                        If False you are responsible for calling .backward, .step, zero_grad. Meant to be used with multiple optimizers by
                        advanced users.
  --model_name_or_path MODEL_NAME_OR_PATH
                        Path to pretrained model or model identifier from huggingface.co/models
  --config_name CONFIG_NAME
                        Pretrained config name or path if not the same as model_name
  --tokenizer_name TOKENIZER_NAME
                        Pretrained tokenizer name or path if not the same as model_name
  --cache_dir CACHE_DIR
                        Where do you want to store the pre-trained models downloaded from huggingface.co
  --encoder_layerdrop ENCODER_LAYERDROP
                        Encoder layer dropout probability (Optional). Goes into model.config
  --decoder_layerdrop DECODER_LAYERDROP
                        Decoder layer dropout probability (Optional). Goes into model.config
  --dropout DROPOUT     Dropout probability (Optional). Goes into model.config
  --attention_dropout ATTENTION_DROPOUT
                        Attention dropout probability (Optional). Goes into model.config
  --learning_rate LEARNING_RATE
                        The initial learning rate for Adam.
  --lr_scheduler {cosine, cosine_w_restarts, linear, polynomial}
                        Learning rate scheduler
  --weight_decay WEIGHT_DECAY
                        Weight decay if we apply some.
  --adam_epsilon ADAM_EPSILON
                        Epsilon for Adam optimizer.
  --warmup_steps WARMUP_STEPS
                        Linear warmup over warmup_steps.
  --num_workers NUM_WORKERS
                        kwarg passed to DataLoader
  --num_train_epochs MAX_EPOCHS
  --train_batch_size TRAIN_BATCH_SIZE
  --eval_batch_size EVAL_BATCH_SIZE
  --adafactor
  --output_dir OUTPUT_DIR
                        The output directory where the model predictions and checkpoints will be written.
  --fp16                Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit
  --fp16_opt_level FP16_OPT_LEVEL
                        For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].See details at
                        https://nvidia.github.io/apex/amp.html
  --n_tpu_cores TPU_CORES
  --max_grad_norm GRADIENT_CLIP_VAL
                        Max gradient norm
  --do_train            Whether to run training.
  --do_predict          Whether to run predictions on the test set.
  --gradient_accumulation_steps ACCUMULATE_GRAD_BATCHES
                        Number of updates steps to accumulate before performing a backward/update pass.
  --seed SEED           random seed for initialization
  --data_dir DATA_DIR   The input data dir. Should contain the training files for the CoNLL-2003 NER task.
  --max_source_length MAX_SOURCE_LENGTH
                        The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,
                        sequences shorter will be padded.
  --max_target_length MAX_TARGET_LENGTH
                        The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,
                        sequences shorter will be padded.
  --val_max_target_length VAL_MAX_TARGET_LENGTH
                        The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,
                        sequences shorter will be padded.
  --test_max_target_length TEST_MAX_TARGET_LENGTH
                        The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,
                        sequences shorter will be padded.
  --freeze_encoder
  --freeze_embeds
  --sortish_sampler
  --overwrite_output_dir
  --max_tokens_per_batch MAX_TOKENS_PER_BATCH
  --logger_name {default,wandb,wandb_shared}
  --n_train N_TRAIN     # examples. -1 means use all.
  --n_val N_VAL         # examples. -1 means use all.
  --n_test N_TEST       # examples. -1 means use all.
  --task TASK           # examples. -1 means use all.
  --label_smoothing LABEL_SMOOTHING
  --src_lang SRC_LANG
  --tgt_lang TGT_LANG
  --eval_beams EVAL_BEAMS
  --val_metric {bleu,rouge2,loss,None}
  --eval_max_gen_length EVAL_MAX_GEN_LENGTH
                        never generate more than n tokens
  --save_top_k SAVE_TOP_K
                        How many checkpoints to save
  --early_stopping_patience EARLY_STOPPING_PATIENCE
                        -1 means never early stop. early_stopping_patience is measured in validation checks, not epochs. So
                        val_check_interval will effect it.